{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_eng\\\\eng_news_2016_1M-co_n.txt', 'data_eng\\\\eng_news_2016_1M-co_s.txt', 'data_eng\\\\eng_news_2016_1M-inv_so.txt', 'data_eng\\\\eng_news_2016_1M-inv_w.txt', 'data_eng\\\\eng_news_2016_1M-sentences.txt', 'data_eng\\\\eng_news_2016_1M-sources.txt', 'data_eng\\\\eng_news_2016_1M-words.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paths = [str(x) for x in Path(\"./data_eng/\").glob(\"**/*.txt\")]\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = []\n",
    "for i in range(1, len(paths)):\n",
    "    df = pd.read_csv(paths[i], sep = \"\\t\", header = None)\n",
    "    dataframe.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            5177\n",
      "1                            6710\n",
      "2                            9894\n",
      "3                           22004\n",
      "4                             343\n",
      "5                           35391\n",
      "6                            1333\n",
      "7                            1649\n",
      "8                            6032\n",
      "9                           31782\n",
      "10                          11949\n",
      "11                          19031\n",
      "12                          18144\n",
      "13                           1842\n",
      "14                          11157\n",
      "15                           1606\n",
      "16                         128631\n",
      "17                             12\n",
      "18                            107\n",
      "19                            982\n",
      "20                            489\n",
      "21                            124\n",
      "22                           2853\n",
      "23                            428\n",
      "24                           2989\n",
      "25                           2250\n",
      "26                           1738\n",
      "27                          33338\n",
      "28                           1665\n",
      "29                          11533\n",
      "                   ...           \n",
      "584970                ﻿Supervisor\n",
      "584971                      ﻿Upon\n",
      "584972                      ﻿With\n",
      "584973                    ﻿﻿After\n",
      "584974                ﻿﻿America’s\n",
      "584975                      ﻿﻿Bob\n",
      "584976                     ﻿﻿Fred\n",
      "584977                       ﻿﻿He\n",
      "584978                      ﻿﻿His\n",
      "584979                    ﻿﻿Nancy\n",
      "584980                        ﻿﻿﻿\n",
      "584981                     ﻿﻿﻿She\n",
      "584982                       ﻿﻿﻿﻿\n",
      "584983                  ﻿﻿﻿﻿After\n",
      "584984           ﻿﻿﻿﻿Arrangements\n",
      "584985                 ﻿﻿﻿﻿Esther\n",
      "584986                 ﻿﻿﻿﻿Leslie\n",
      "584987                ﻿﻿﻿﻿﻿﻿Diana\n",
      "584988                  ﻿﻿﻿﻿﻿﻿Roy\n",
      "584989                  ﻿﻿﻿﻿﻿﻿She\n",
      "584990    ﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿LEO\n",
      "584991           ，冒着枪林弹雨为战友们运送弹药。\n",
      "584992                    ，除了whip\n",
      "584993                   ：二战中最传奇的\n",
      "584994                       ￡217\n",
      "584995                    ￥102.76\n",
      "584996                         ￥9\n",
      "584997                        ￥9m\n",
      "584998                     ￼World\n",
      "584999              ￼￼￼￼￼Aspiring\n",
      "Name: 1, Length: 31203363, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "overall_df = pd.concat(dataframe)\n",
    "overall_df = overall_df[1]\n",
    "print(overall_df)\n",
    "\n",
    "overall_df.to_csv('new_data_eng.txt', header=None, index=None, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5177\n",
       "1     6710\n",
       "2     9894\n",
       "3    22004\n",
       "4      343\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2 = \"\" \n",
    "  \n",
    "# # Reading data from file1 \n",
    "with open('new_data_eng.txt',encoding='utf-8') as fp: \n",
    "    data = fp.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training BPE Tokeniser\n",
    "Here, the goal is to build two files: vocab.json and merges.txt. The vocab.json is a list of the top K tokens found in the text corpus that you built in the previous step map to their respective token ids. Since we are building a byte pair encoding (BPE) tokeniser, the merges.txt allows us to perform subword tokenisation on our input text. We would be using the tokenizer from the Hugging Face library as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "data = 'new_data_eng.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer1 = ByteLevelBPETokenizer(dropout=0.1)\n",
    "\n",
    "# Customize training\n",
    "tokenizer1.train(files=data, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./english\\\\vocab.json', './english\\\\merges.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer1.save(\"./english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout = 0.1\n",
      "['There', 'Ġare', 'Ġa', 'Ġlot', 'Ġof', 'Ġapples', 'Ġin', 'Ġt', 'he', 'Ġgarden']\n",
      "['There', 'Ġare', 'Ġa', 'Ġlot', 'Ġof', 'Ġapples', 'Ġin', 'Ġthe', 'Ġgard', 'en']\n",
      "['There', 'Ġare', 'Ġa', 'Ġlot', 'Ġof', 'Ġap', 'ples', 'Ġin', 'Ġth', 'e', 'Ġgarden']\n",
      "['There', 'Ġare', 'Ġa', 'Ġlo', 't', 'Ġof', 'Ġapples', 'Ġin', 'Ġthe', 'Ġgarden']\n",
      "['Th', 'ere', 'Ġare', 'Ġa', 'Ġlot', 'Ġof', 'Ġapples', 'Ġin', 'Ġthe', 'Ġgarden']\n",
      "['Th', 'ere', 'Ġar', 'e', 'Ġa', 'Ġ', 'lot', 'Ġof', 'Ġap', 'ples', 'Ġin', 'Ġth', 'e', 'Ġg', 'arden']\n",
      "['There', 'Ġare', 'Ġ', 'a', 'Ġ', 'lot', 'Ġof', 'Ġapples', 'Ġin', 'Ġthe', 'Ġg', 'arden']\n",
      "['There', 'Ġare', 'Ġa', 'Ġl', 'ot', 'Ġof', 'Ġapples', 'Ġin', 'Ġthe', 'Ġgarden']\n",
      "['T', 'her', 'e', 'Ġare', 'Ġa', 'Ġlot', 'Ġo', 'f', 'Ġapples', 'Ġi', 'n', 'Ġthe', 'Ġgarden']\n",
      "['There', 'Ġare', 'Ġa', 'Ġlot', 'Ġ', 'of', 'Ġap', 'ples', 'Ġin', 'Ġthe', 'Ġgarden']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bpe_tokeniser_from_files = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file='english/vocab.json',\n",
    "    merges_file='english/merges.txt',\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print('dropout =', bpe_tokeniser_from_files._parameters['dropout'])\n",
    "for _ in range(10):\n",
    "    print(bpe_tokeniser_from_files.encode('There are a lot of apples in the garden').tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1997, 531, 265, 434, 395, 361, 32370, 225, 263, 262, 264, 9411, 364, 626, 531, 404, 16017, 18] ['There', 'Ġare', 'Ġa', 'Ġl', 'ot', 'Ġof', 'Ġapples', 'Ġ', 'in', 'Ġt', 'he', 'Ġgarden', 'Ġand', 'Ġwe', 'Ġare', 'Ġg', 'lad', '.'] [(0, 5), (5, 9), (9, 11), (11, 13), (13, 15), (15, 18), (18, 25), (25, 26), (26, 28), (28, 30), (30, 32), (32, 39), (39, 43), (43, 46), (46, 50), (50, 52), (52, 55), (55, 56)]\n"
     ]
    }
   ],
   "source": [
    "output = bpe_tokeniser_from_files.encode(\"There are a lot of apples in the garden and we are glad.\")\n",
    "print(output.ids, output.tokens, output.offsets)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=18, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
